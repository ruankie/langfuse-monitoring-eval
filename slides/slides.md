---
marp: true
theme: default
class: invert
# size: 16:9
footer: Ruan Pretorius | October 2024 | ![grayscale height:15](../assets/melio-logo.svg)
style: |
  section {
    font-size: 32px;
  }
---

![bg left:35% height:300px](../assets/pyconza.png)

# Monitoring and Evaluating LLM Apps
> *Using Langfuse*

---

## About me

##### Hi there, I'm Ruan Pretorius ğŸ‘‹

- â˜• I turn coffee into AI
- ğŸ–¥ I am a data scientist at *[melio.ai](https://melio.ai/)*
  - We help you build and deploy your data intensive apps to unlock value from your data, follow us on LinkedIn
- ğŸ”— You can find me on GitHub *[@ruankie](https://github.com/ruankie)*
- âœ‰ï¸ Or contact me via email: *ruan@melio.ai*

#

![height:50](../assets/melio-logo.svg)

---

## ğŸš Outline

- **Introduction**: Why monitor/evaluate LLM apps?
- **Setup**: Local LLMs for prototyping
- **Monitoring**: with Langfuse
- **Evaluation**: LLM-assisted with Ollama and Langfuse
- **Takeaways and Conclusion**

---

## â“ Why Monitor & Evaluate LLM Apps?

- **Ensure Quality & Performance**
  - Track hallucination, retrieval accuracy, latency, etc.
  - To maintain a high-quality user experience
- **Detect Errors**
  - Harmful outputs
- **Identify Areas of Improvement**
  - Reduce costs
  - Reduce latency
  - Improve answers
  - If failure occurs, see when and where

---

## âš¡ Setting Up

> Local LLMs for zero-cost learning and prototyping

### ğŸ¦™ Ollama

- For locally running LLMs
- Available for macOS, Linux, and Windows (preview)
- Familiar Docker feel with `:version` tags and commands like `pull` and `run`

![bg right:30% height:250px](../assets/ollama.png)

---

## ğŸ¦™ Ollama Setup

- â¬‡ï¸ Download app from [`https://ollama.com/`](https://ollama.com/)
- Download LLM of choice

  ```shell
  ollama pull llama3.1:8b
  ```

- To test, run LLM in terminal

  ```shell
  ollama run llama3.1:8b
  ```
  
![bg right:30% height:250px](../assets/ollama.png)

---

## ğŸ§° Monitoring LLM Apps with Langfuse

- **What is Langfuse?** ğŸš€
  - Open-source platform for LLM tracing and evaluation
  - Focus on detailed app performance metrics ğŸ“Š
- **Why Monitor?**
  - Spot issues before they impact users ğŸ”
  - Ensure consistent LLM performance ğŸ“ˆ
---

## ğŸ—ï¸ Setting Up Langfuse

- **Installing Langfuse** ğŸ› ï¸
  - Installation guide
  - Integrating Langfuse with Python SDK ğŸ
- **Configuring Langfuse for Tracing**
  - Key configurations for monitoring and logs ğŸ“
  - Setting up evaluation datasets ğŸ“š
---

## ğŸ¯ Monitoring with Langfuse: Live Example

- **Using Langfuse's Python Decorator** ğŸ”„
  - Tracing requests and responses in LLM apps
  - Real-time dashboard view ğŸ“Š
- **Example Implementation**:
  - Simple Langfuse integration in a Python LLM app
---

## ğŸ“Š LLM-Assisted Evaluation with Langfuse

- **Evaluation Datasets** ğŸ“š
  - How Langfuse helps score and evaluate your LLM outputs
- **LLM-Assisted Scoring** ğŸ¯
  - Automated evaluation using predefined metrics
  - Feedback loop to improve LLM performance
---

## ğŸ“ˆ Best Practices for Monitoring and Evaluation

- **Key Takeaways** ğŸ’¡
  - Prototyping fast and free with local LLMs ğŸ–¥ï¸
  - Monitoring with Langfuse to ensure app robustness ğŸ“Š
  - LLM-assisted evaluation for continuous improvement ğŸ”
---

## ğŸ‰ Conclusion: Elevate Your LLM Workflow

- **Summary** of key points:
  - Local LLM setup with Ollama ğŸš€
  - Langfuse for monitoring and evaluation ğŸ§°
  - Automating scoring and feedback ğŸ“
- **Final Thoughts**: Start small, scale smart! ğŸŒŸ
---

#

## <!--fit--> ğŸ Thank you!

#

- ğŸ”— GitHub: *[@ruankie](https://github.com/ruankie)*
- âœ‰ï¸ Email: *ruan@melio.ai*
- ğŸ  Melio website: *[melio.ai](https://melio.ai)*

  ![height:50](../assets/melio-logo.svg)